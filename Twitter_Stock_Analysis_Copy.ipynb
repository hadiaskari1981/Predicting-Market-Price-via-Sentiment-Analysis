{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy import Cursor\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from __future__ import print_function\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stop = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv('./training_manutd.csv', header= None, names= cols, encoding = \"ISO-8859-1\")\n",
    "#df.to_json('./training_manutd.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "df.sentiment.value_counts()\n",
    "# 1 Posative 0 Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.str.replace(r'\\d+', '')\n",
    "df.text = df['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        stopwords=stop,\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        collocations = False,\n",
    "        scale=15,\n",
    "        random_state=42 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(25, 25))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist = nltk.FreqDist(df['text'])\n",
    "word_dist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'date', 'query_string', 'user'], axis = 1, inplace= True)\n",
    "label = pd.factorize(df['sentiment'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],label, train_size=.8,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X_train = vec.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MultinomialNB()\n",
    "cls.fit(x_train_tfidf , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(X_test)\n",
    "x_test_tfidf = tfidf_transformer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls.predict(x_test_tfidf))\n",
    "x_test_pred = cls.predict(x_test_tfidf)\n",
    "x = x_test_pred.reshape(1, -1)\n",
    "y = y_test.reshape(1, -1)\n",
    "print(y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.score(x_test_tfidf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "####input your credentials here\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "consumer_key = 'RR6miKQ2GKE4cSKj889737iMr'\n",
    "consumer_secret = 'FLLMME41CmVgCRGrydyoBFR3cmJWTpAmkndyjJI29NvkdYXlrE'\n",
    "access_token = '2520042060-gTmuDa7yaKLWcJhJLq5igkwckTFBdqCBT9OopIh'\n",
    "access_token_secret = 'tm6UVzoCPPO7L9Hiq9blymj0pVb5v0YMVEZdl2DLnaVLp'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "#####United Airlines\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('TweetsAfter_27_03_2018.csv', 'a', encoding=\"utf-8\")\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#ManUtd\",count=5000,\n",
    "                           lang=\"en\",since=\"2017-04-03\").items():\n",
    "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8'),tweet.retweets])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd = pd.read_csv('./Tweets.csv', header = None, names = ['date_time', 'text'],encoding=\"ISO-8859-1\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd['Orig_text'] = df_manutd['text']\n",
    "df_manutd.text = df_manutd.text.str.replace(r'\\d+', '')\n",
    "df_manutd.text = df_manutd['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "df_manutd['text'] = df_manutd['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))\n",
    "df_manutd['text'] = df_manutd['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd.text = df_manutd.text.str.replace(r'\\d+', '')\n",
    "df_manutd.text = df_manutd.text.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'xe', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'xf', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'xb', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'xa', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'//', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'\\\\', '', case=False)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'mx', '', case=False)\n",
    "#m = re.sub(r':.*$', \":\", str)\n",
    "df_manutd.text = df_manutd.text.str.replace(r'brt', '', case=False)\n",
    "df_manutd.text = df_manutd['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "df_manutd['text'] = df_manutd['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stop)]))\n",
    "\n",
    "#ttnz = TweetTokenizer()\n",
    "\n",
    "#f_manutd['tokenized_tweets'] = df_manutd['text'].apply(ttnz.tokenize)\n",
    "\n",
    "#n_gram = df_manutd['text'].apply(ngrams(nltk.word_tokenize,n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnz = TweetTokenizer()\n",
    "\n",
    "df_manutd['tokenized_tweets'] = df_manutd['text'].apply(ttnz.tokenize)\n",
    "\n",
    "stemmer = SnowballStemmer(language= 'english', ignore_stopwords= True)\n",
    "df_manutd['tokenized_tweets'] = df_manutd['tokenized_tweets'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(10)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_manutd['tokenized_tweets'] = df_manutd['tokenized_tweets'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = df_manutd.text.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "top_N = 30\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n",
    "\n",
    "print('All frequencies, including STOPWORDS:')\n",
    "print('=' * 60)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "ax = sns.barplot(x='Word',y='Frequency',data=rslt,palette=sns.light_palette(\"navy\",reverse=True,n_colors=21))\n",
    "plt.xticks(rotation=70)\n",
    "plt.xticks(size=15)\n",
    "plt.yticks(size=15)\n",
    "ax.set(ylim=(0,6000))\n",
    "ax.set(xlim=(None,20))\n",
    "ax.set_xlabel('Wrods',size=17)\n",
    "ax.set_ylabel('Frequency',size=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        stopwords=stop,\n",
    "        max_words=150,\n",
    "        max_font_size=60, \n",
    "        collocations = False,\n",
    "        scale=10,\n",
    "        random_state=42 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(16, 16))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(df_manutd['tokenized_tweets'])\n",
    "#show_wordcloud(Samsung_Reviews_positive['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd_text = vec.transform(df_manutd.text)\n",
    "df_manutd_text_tfidf = tfidf_transformer.fit_transform(df_manutd_text)\n",
    "df_pred = cls.predict_proba(df_manutd_text_tfidf)\n",
    "df_pred_bin = cls.predict(df_manutd_text_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(df_pred)\n",
    "df_pred_bin = pd.DataFrame(df_pred_bin)\n",
    "df_pred_bin = df_pred_bin.reset_index()\n",
    "df_pred.columns = [['prediction_p', 'prediction_n']]\n",
    "df_pred = df_pred.reset_index()\n",
    "df_pred_bin['isPos'] = df_pred_bin[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manutd_merge = df_manutd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred_bin.merge(df_pred, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.merge(df_manutd_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.drop(df_pred.columns[[0,1,3]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pred = df_pred.join(df_manutd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.drop(['text', 'tokenized_tweets'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.date_time = pd.to_datetime(df_pred.date_time)\n",
    "df_pred.date_time = df_pred.date_time.dt.date\n",
    "df_pred = df_pred.groupby('date_time').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "\n",
    "params = [\n",
    "    # Dow Jones\n",
    "    {\n",
    "        'q': \"MANU\",\n",
    "        'x': \"NYSE\",\n",
    "    },\n",
    "]\n",
    "period = \"1M\"\n",
    "interval = 60*60*24 # 30 minutes\n",
    "# get open, high, low, close, volume time data (return pandas dataframe)\n",
    "df_stock = get_prices_time_data(params, period, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.index.names = ['date_time']\n",
    "df_stock.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.date(2018, 3, 15)\n",
    "df_stock = df_stock[df_stock['date_time'] > date]\n",
    "df_stock.date_time = pd.to_datetime(df_stock.date_time)\n",
    "df_stock.date_time = df_stock.date_time.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_stock.merge(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "t = df['date_time']\n",
    "s1 = df['MANU_High']\n",
    "ax1.plot(t, s1, 'b-')\n",
    "ax1.set_xlabel('Date', size = 15)\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Stock Price', color='b', size =15)\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# s2 = df['prediction_p']\n",
    "# ax2.plot(t, s2, 'r-')\n",
    "# ax2.set_ylabel('Positve Sentiment', color='r' , size = 15)\n",
    "# ax2.tick_params('y', colors='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "s2 = df['isPos']\n",
    "ax2.plot(t, s2, 'g-')\n",
    "ax2.set_ylabel('Positve Sentiment', color='g' , size = 15)\n",
    "ax2.tick_params('y', colors='g')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
